{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10ae65610> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10ae67dd0> root_client=<openai.OpenAI object at 0x104273e90> root_async_client=<openai.AsyncOpenAI object at 0x10ae657d0> model_name='gpt-4-turbo-preview' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Generative AI, or generative artificial intelligence, refers to a subset of AI technologies and algorithms designed to generate new content, including but not limited to text, images, videos, and music, that resemble human-like creations. Unlike discriminative models that are used for classification tasks (distinguishing between different types of data), generative AI aims to create or generate data instances that can be entirely new, yet plausible based on learning from a set of input data.\\n\\nThis capability is achieved through various machine learning techniques such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models. Here's a brief overview of some of these techniques:\\n\\n1. **Generative Adversarial Networks (GANs):** GANs consist of two neural networks, a generator and a discriminator, which are trained simultaneously through adversarial processes. The generator learns to create data that is indistinguishable from real data, while the discriminator learns to distinguish between the generator's fake data and real data. Through this process, the generator improves over time to produce highly realistic outputs.\\n\\n2. **Variational Autoencoders (VAEs):** VAEs are designed to encode input data into a compressed representation and then decode that representation back into the original data. During this process, they learn the parameters of the probability distribution representing the data, enabling them to generate new data points by sampling from this distribution.\\n\\n3. **Transformer-based Models:** Originally designed for natural language processing tasks, transformer models like GPT (Generative Pre-trained Transformer) and its successors (e.g., GPT-2, GPT-3) have shown remarkable ability to generate coherent and contextually relevant text based on given prompts. These models have also been adapted for generating images (e.g., DALL-E) and music.\\n\\nGenerative AI has applications across a wide range of fields, from content creation and digital art, game design, drug discovery, and more. It holds the promise to automate creative processes, assist in design and innovation, and personalize user experiences. However, it also raises ethical considerations, such as copyright issues, potential misuse for creating realistic deepfakes, and the impact on creative industries.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 448, 'prompt_tokens': 13, 'total_tokens': 461, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0125-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e82d5ebc-2085-4853-857b-35ff2bcd2113-0', usage_metadata={'input_tokens': 13, 'output_tokens': 448, 'total_tokens': 461, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result = llm.invoke(\"What is generative AI?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "# Import ChatPromptTemplate to create structured chat prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a chat prompt template with system and user messages\n",
    "# The system message sets the AI's role and behavior\n",
    "# The user message contains a placeholder {input} that will be filled in later\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",  # System message defines the AI's role\n",
    "            \"You are an expert AI Engineer. Provide me answers based on the questions\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),  # User message with placeholder for dynamic input\n",
    "    ]\n",
    ")\n",
    "# Display the prompt template\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As of my last update in December 2023, Langsmith refers to a language model developed by researchers at the University of California, Berkeley. It is designed to assist in writing academic papers, specifically targeting the field of computational linguistics. One of its notable features is its capability to generate text based on a minimal amount of input from the user, such as a brief description or a set of keywords. \\n\\nLangsmith aims to streamline the academic writing process, reducing the time and effort researchers spend on drafting papers. However, it's important to note the ethical considerations and the need for careful review when using AI-generated content in academic works. Langsmith is a part of the broader trend of leveraging advanced AI and machine learning techniques to enhance productivity and creativity in various fields, including academic research.\\n\\nPlease note that the description provided here is based on the state of information available up until December 2023. There might be updates or changes to Langsmith after this time.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 33, 'total_tokens': 227, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0125-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-29726f3e-c33a-465e-b397-939228d1cd18-0', usage_metadata={'input_tokens': 33, 'output_tokens': 194, 'total_tokens': 227, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chain\n",
    "# Create a chain by combining the prompt template and LLM using the | operator\n",
    "# The | operator pipes the output of one component into the input of the next\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain with an input dictionary containing the question\n",
    "# The {input} placeholder in the prompt will be replaced with this question\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "\n",
    "# Print the response from the LLM\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! As of my knowledge cutoff date in October 2023, \"Langsmith\" does not directly correlate to any widely recognized technology, framework, or concept in the field of AI or software engineering. It's possible that it might be a term used within a specific company, a proprietary tool, or a recent development that hasn't gained widespread recognition.\n",
      "\n",
      "However, if \"Langsmith\" is a term related to a specific context or a new product or service that emerged after my last update, I wouldn't have information on it. If you could provide more context or details, I'd be happy to help you understand the concept or direct you to the right resources!\n"
     ]
    }
   ],
   "source": [
    "## String Output Parser\n",
    "# Import the StrOutputParser to convert LLM responses to plain strings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a string output parser instance\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain that:\n",
    "# 1. Takes the prompt template\n",
    "# 2. Passes it to the LLM\n",
    "# 3. Converts the LLM response to a plain string using the output parser\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain with a question and print the response\n",
    "# The response will now be a plain string instead of an AIMessage object\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
