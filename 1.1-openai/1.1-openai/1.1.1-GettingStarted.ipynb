{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10f113d90> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10b430910> root_client=<openai.OpenAI object at 0x104287150> root_async_client=<openai.AsyncOpenAI object at 0x10a7d4890> model_name='gpt-4-turbo-preview' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Generative AI, or generative artificial intelligence, refers to a subset of AI technologies that can create new content or data that is similar to but not identical to data it has been trained on. This includes generating text, images, videos, music, voice, and even code. The capabilities of generative AI come from different algorithmic techniques, most notably deep learning models like Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformers.\\n\\nGenerative AI models work by learning patterns, features, and structures from large datasets during a training phase. Once trained, they can generate new, original outputs based on the inputs they receive, which are often guided by parameters defined by a user. For example:\\n\\n1. **Text:** Generative AI can produce coherent and contextually relevant passages of text. OpenAI's GPT (Generative Pretrained Transformer) series is a notable example, being widely used for various applications such as chatbots, content creation, and programming assistance.\\n\\n2. **Images:** With GANs and other models, generative AI can create realistic images or artwork from textual descriptions or by modifying existing images. This has applications in game development, design, and even in creating deepfake media.\\n\\n3. **Music and Voice:** Generative AI can compose music or generate realistic human-like speech from text. These capabilities allow for innovations in entertainment, content creation, and accessibility technologies.\\n\\n4. **Code:** AI models can also generate snippets of code based on descriptions in natural language, aiding developers by automating some of the coding tasks.\\n\\nGenerative AI is at the forefront of technological advancement due to its potential to innovate and automate in creative fields, significantly impacting industries like entertainment, arts, software development, and marketing. However, it also raises ethical considerations regarding copyright, authenticity, and misinformation, necessitating discussions and policies around its use.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 381, 'prompt_tokens': 13, 'total_tokens': 394, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0125-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1b06449a-792d-489c-82df-642a55b9c337-0', usage_metadata={'input_tokens': 13, 'output_tokens': 381, 'total_tokens': 394, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result = llm.invoke(\"What is generative AI?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "# Import ChatPromptTemplate to create structured chat prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a chat prompt template with system and user messages\n",
    "# The system message sets the AI's role and behavior\n",
    "# The user message contains a placeholder {input} that will be filled in later\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",  # System message defines the AI's role\n",
    "            \"You are an expert AI Engineer. Provide me answers based on the questions\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),  # User message with placeholder for dynamic input\n",
    "    ]\n",
    ")\n",
    "# Display the prompt template\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of my last update in December 2023, \"Langsmith\" doesn\\'t have a widely recognized definition or reference that I can provide without more context. The name could refer to a variety of things depending on the domain in question, such as a brand, technology, software, or even a surname. In the realm of languages, technology, or software development, \"Langsmith\" could potentially be the name of a programming language, a tool, an application, or a service related to language processing, translation, or learning. However, without more specific details, it\\'s challenging to provide a precise answer.\\n\\nGiven the pace at which technology and startups evolve, it\\'s possible that \"Langsmith\" might emerge as a new tool or company in the fields of linguistics, software development, or another area entirely. If you have a more specific context or domain where Langsmith is used, I could try to provide more tailored information.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 189, 'prompt_tokens': 33, 'total_tokens': 222, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0125-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4a04dbbf-2ee1-4b0c-a464-845b0ea63de8-0', usage_metadata={'input_tokens': 33, 'output_tokens': 189, 'total_tokens': 222, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chain\n",
    "# Create a chain by combining the prompt template and LLM using the | operator\n",
    "# The | operator pipes the output of one component into the input of the next\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain with an input dictionary containing the question\n",
    "# The {input} placeholder in the prompt will be replaced with this question\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "\n",
    "# Print the response from the LLM\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! As of my knowledge cutoff date in October 2023, \"Langsmith\" does not directly correlate to any widely recognized technology, framework, or concept in the field of AI or software engineering. It's possible that it might be a term used within a specific company, a proprietary tool, or a recent development that hasn't gained widespread recognition.\n",
      "\n",
      "However, if \"Langsmith\" is a term related to a specific context or a new product or service that emerged after my last update, I wouldn't have information on it. If you could provide more context or details, I'd be happy to help you understand the concept or direct you to the right resources!\n"
     ]
    }
   ],
   "source": [
    "## String Output Parser\n",
    "# Import the StrOutputParser to convert LLM responses to plain strings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a string output parser instance\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain that:\n",
    "# 1. Takes the prompt template\n",
    "# 2. Passes it to the LLM\n",
    "# 3. Converts the LLM response to a plain string using the output parser\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain with a question and print the response\n",
    "# The response will now be a plain string instead of an AIMessage object\n",
    "response = chain.invoke({\"input\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
