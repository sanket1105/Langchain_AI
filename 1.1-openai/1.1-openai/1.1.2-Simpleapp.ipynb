{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Gen AI APP Using Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x11a8b63d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    \"https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d\"\n",
    ")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | MediumOpen in appSign upSign inWriteSign upSign inTracing How LangChain Works Behind the ScenesKelvin Lu·Follow7 min read·Jun 26, 2023--ListenShareHave you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a hallucination? How to tell if a result results from prudent reasoning or a devilish joke?René DescartesFour hundred years ago, French mathematician and philosopher René Descartes worried that if the foundations of knowledge were not completely solid, anything built upon them would inevitably collapse. He thus decided that if there was reason to doubt the truth of something, no matter how slim the doubt, then it should be discarded as false. Descartes’s method of doubt was radical, but it led him to a famous conclusion: “I think, therefore I am.” The only thing that Descartes could be certain of was his own existence, because even if he doubted everything else, he still had to exist in order to do the doubting.The old wisdom of Descartes’s method of doubt is still relevant in the era of generative AI (GAI). We know that GAI can hallucinate, and we don’t want to fully trust the machine. Therefore, we should be skeptical of GAI’s thoughts and outputs, and we should verify its information. By doing so, we can ensure that we are using GAI responsibly and safely.Tracing is a way to see the inner workings of a GAI model. By tracing LangChain, you can see how it arrives at its answers. This will help you understand how the model works and trust the answers it gives you. In this post, I’ll show you how to trace LangChain in two different ways.The Baseline Test CaseWe have designed a simple function that can solve a complex problem: how long does it take for the fastest golf ball to travel around the Earth?The function is composed of two parts: the SERP API and LLM_Math. LLM_Math is a wrapper of ChatGPT that can do simple math calculations, while the SERP API is an interface that allows Google search.When the function is asked a question, it first tries to find the answer in its knowledge base. If it can’t find the answer, it then uses the SERP API to search Google for supporting information. If it finds the information on Google, it then uses LLM_Math to calculate the final answer.Here is a code snippet that shows how the function works:from langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIdef search_agent_demo():    llm = OpenAI(temperature=0)    tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)    agent = initialize_agent(        tools, llm, agent=\"zero-shot-react-description\", verbose=True    )    return agent.run(        \"How long does the fastest golf ball travel around the Earth?\\\\n\"        \" - Do you know the speed of the fastest golf ball? Find it out.\\\\n\"        \" - You also need to figure out the circumference of the Earth as well.\\\\n\"        \" - Final answer should be in hours.\"    )search_agent_demo()The code requires calling the OpenAI and SERP APIs. To run the code, you will need to set up two environment variables: OPENAI_API_KEY and SERPAPI_API_KEY. You can generate your own API keys on the OpenAI: https://beta.openai.com/account/api-keys and SERPAPI: https://serpapi.com/ websites.The following is an example of the output of the code:Running resultWe turned on the verbose agent run mode to see the steps of the agent’s thoughts. The output showed that the agent first checked its knowledge base to see if it knew the speed of the fastest golf ball and the circumference of the Earth. When it didn’t find the answer, it used the SERP API to search Google for the information. It then calculated the answer and outputted it.The entire process looked solid, and the results looked good. The agent could find the information it needed and calculate the answer correctly. However, not all experiments will run as smoothly as this one.In some cases, the agent may stray from the correct path. In these cases, the verbose mode may not produce enough information to help us understand why the agent went astray. That’s why we need tracing to zoom in and see what the agent is doing at each step.Tracing with LangChain Built-in FunctionOne way is to use the LangChain’s native tracing support. LangChain, the very popular and rapidly developing framework, does support tracing. However, the document of tracing is incomplete. I hope the following guidelines can save you some time.Step 0: prepare dependenciesLangChain tracing uses a web-server to collect agent-run information. The web-server uses port number 8000 to collect trace information and port number 4173 to serve the user interface. The web-server runs as a Docker container. Before we go any further, please make sure the langchain has been installed, Docker has been installed, and the command docker-compose is executable.Step 1: start the tracing serverOnce the environment is ready, just run the following command to start the tracing server:python -m langchain.serverThis command will build a langchain server container and spin it up.Running LangChain ServerStep 2: enable tracing and run the payloadThere are two ways to enable tracing. The first option is to use the environment variable:os.environ[“LANGCHAIN_TRACING”] = “true”The second way is to not use the environment variable but to apply with-statement to your code snippet. In the tracing_enabled() function call, you can optionally set the session name:from langchain.callbacks import tracing_enabled…with tracing_enabled(‘sess_test_tracing’) as session: assert session search_agent_demo()Step 3: inspect the trace informationIf we apply either of the options and re-run the code, we can opent the webui from the address:http://localhost:4173/sessionsThe Webui looks as the following:When we invoked the function tracing_enabled(), we optionally provided a session name: sess_test_tracing, otherwise, all the tracing information went into the default session. The tracing server doesn’t automatically detect session names. We have to manually create a new session named ‘sess_test_tracing’ to match the session name in the code.When we open the session, we can find the tracing details:If we click on the ‘Explore’ buttons, we can find all the prompt information for each interaction. For example:Answer the following questions as best you can. You have access to the following tools:Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.Calculator: Useful for when you need to answer questions about math.Use the following format:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [Search, Calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: How long does the fastest golf ball travel around the Earth? - Do you know the speed of the fastest golf ball? Find it out. - You also need to find out the circumference of the Earth as well. - Final answer should be in hours.Thought:Without the help of tracing, we would have to look into the source code to know these details.Tracing with LangChain VisualizerBesides the LangChain tracing, I would also recommend an alternative — LangChain Visualizer. The reasons are that the LangChain tracing requires running in Docker, and that the UI functions are not well refined.Step 0: installationpip install langchain-visualizerStep 1: change the codeLangChain visualizer requires the payload to be wrapped in an async function.# from langchain_visualizer import visualize # use this when not run in Jupyterfrom langchain_visualizer.jupyter import visualizefrom langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIasync def search_agent_demo():    llm = OpenAI(temperature=0)    tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)    agent = initialize_agent(        tools, llm, agent=\"zero-shot-react-description\", verbose=True    )    return agent.run(        \"How long does a plane travel around the Earth at sonic speed?\"        \" - Remember the unit of circumference of the Earth is in kilometer while the unit of sound speed is meter per second.\"        \" - You also need to convert the kilometers into meters in your calculation and the final answer should be in hours.\"        \" - Do your reasoning independently.\"     )# you don\\'t have to specify width and height# but if you do, you can change the size of the rendered windowvisualize(search_agent_demo, width=1000, height=500)The above code snippet runs in Jupyter Notebook. The result of the code run is the following: You can choose to open the link in the web browser to access the same outcome.I found LangChain Visualizer to be more elegant, and I liked the cost estimation it produced for each call.ConclusionGenerative AI (GAI) is still in its early stages of development. However, I believe that we will see more and more sophisticated applications of GAI in the near future. As these applications become more complex, it will become increasingly important for us to understand how they work.In addition to debugging, tracing can be used to improve the performance of GAI systems. By identifying areas where the system can be improved, we can make it more efficient and effective.More importantly, tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing is a valuable tool for engineers who are working with GAI systems. I hope you find spending time on it worthwhile.LangchainLlmGenerative Ai ToolsOpenAIChatGPT----FollowWritten by Kelvin Lu958 Followers·291 FollowingSydney-based ML engineer / data engineer @ InteliaFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | MediumOpen in appSign upSign inWriteSign upSign inTracing How LangChain Works Behind the ScenesKelvin Lu·Follow7 min read·Jun 26, 2023--ListenShareHave you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a hallucination? How to tell if a result results from prudent reasoning or a devilish joke?René DescartesFour hundred years ago, French mathematician and philosopher René Descartes worried that if the foundations of knowledge were not completely solid, anything built upon them would inevitably collapse. He thus decided that if there was reason to doubt the truth of something, no matter how slim the doubt, then it should be discarded as false. Descartes’s method of doubt was radical, but it led him to a famous conclusion: “I think, therefore I am.” The only thing that Descartes could be certain'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='doubt, then it should be discarded as false. Descartes’s method of doubt was radical, but it led him to a famous conclusion: “I think, therefore I am.” The only thing that Descartes could be certain of was his own existence, because even if he doubted everything else, he still had to exist in order to do the doubting.The old wisdom of Descartes’s method of doubt is still relevant in the era of generative AI (GAI). We know that GAI can hallucinate, and we don’t want to fully trust the machine. Therefore, we should be skeptical of GAI’s thoughts and outputs, and we should verify its information. By doing so, we can ensure that we are using GAI responsibly and safely.Tracing is a way to see the inner workings of a GAI model. By tracing LangChain, you can see how it arrives at its answers. This will help you understand how the model works and trust the answers it gives you. In this post, I’ll show you how to trace LangChain in two different ways.The Baseline Test CaseWe have designed a'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='This will help you understand how the model works and trust the answers it gives you. In this post, I’ll show you how to trace LangChain in two different ways.The Baseline Test CaseWe have designed a simple function that can solve a complex problem: how long does it take for the fastest golf ball to travel around the Earth?The function is composed of two parts: the SERP API and LLM_Math. LLM_Math is a wrapper of ChatGPT that can do simple math calculations, while the SERP API is an interface that allows Google search.When the function is asked a question, it first tries to find the answer in its knowledge base. If it can’t find the answer, it then uses the SERP API to search Google for supporting information. If it finds the information on Google, it then uses LLM_Math to calculate the final answer.Here is a code snippet that shows how the function works:from langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIdef search_agent_demo():    llm ='),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='the final answer.Here is a code snippet that shows how the function works:from langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIdef search_agent_demo():    llm = OpenAI(temperature=0)    tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)    agent = initialize_agent(        tools, llm, agent=\"zero-shot-react-description\", verbose=True    )    return agent.run(        \"How long does the fastest golf ball travel around the Earth?\\\\n\"        \" - Do you know the speed of the fastest golf ball? Find it out.\\\\n\"        \" - You also need to figure out the circumference of the Earth as well.\\\\n\"        \" - Final answer should be in hours.\"    )search_agent_demo()The code requires calling the OpenAI and SERP APIs. To run the code, you will need to set up two environment variables: OPENAI_API_KEY and SERPAPI_API_KEY. You can generate your own API keys on the OpenAI: https://beta.openai.com/account/api-keys and SERPAPI: https://serpapi.com/ websites.The'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='environment variables: OPENAI_API_KEY and SERPAPI_API_KEY. You can generate your own API keys on the OpenAI: https://beta.openai.com/account/api-keys and SERPAPI: https://serpapi.com/ websites.The following is an example of the output of the code:Running resultWe turned on the verbose agent run mode to see the steps of the agent’s thoughts. The output showed that the agent first checked its knowledge base to see if it knew the speed of the fastest golf ball and the circumference of the Earth. When it didn’t find the answer, it used the SERP API to search Google for the information. It then calculated the answer and outputted it.The entire process looked solid, and the results looked good. The agent could find the information it needed and calculate the answer correctly. However, not all experiments will run as smoothly as this one.In some cases, the agent may stray from the correct path. In these cases, the verbose mode may not produce enough information to help us understand why the'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='will run as smoothly as this one.In some cases, the agent may stray from the correct path. In these cases, the verbose mode may not produce enough information to help us understand why the agent went astray. That’s why we need tracing to zoom in and see what the agent is doing at each step.Tracing with LangChain Built-in FunctionOne way is to use the LangChain’s native tracing support. LangChain, the very popular and rapidly developing framework, does support tracing. However, the document of tracing is incomplete. I hope the following guidelines can save you some time.Step 0: prepare dependenciesLangChain tracing uses a web-server to collect agent-run information. The web-server uses port number 8000 to collect trace information and port number 4173 to serve the user interface. The web-server runs as a Docker container. Before we go any further, please make sure the langchain has been installed, Docker has been installed, and the command docker-compose is executable.Step 1: start the'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='runs as a Docker container. Before we go any further, please make sure the langchain has been installed, Docker has been installed, and the command docker-compose is executable.Step 1: start the tracing serverOnce the environment is ready, just run the following command to start the tracing server:python -m langchain.serverThis command will build a langchain server container and spin it up.Running LangChain ServerStep 2: enable tracing and run the payloadThere are two ways to enable tracing. The first option is to use the environment variable:os.environ[“LANGCHAIN_TRACING”] = “true”The second way is to not use the environment variable but to apply with-statement to your code snippet. In the tracing_enabled() function call, you can optionally set the session name:from langchain.callbacks import tracing_enabled…with tracing_enabled(‘sess_test_tracing’) as session: assert session search_agent_demo()Step 3: inspect the trace informationIf we apply either of the options and re-run the'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='import tracing_enabled…with tracing_enabled(‘sess_test_tracing’) as session: assert session search_agent_demo()Step 3: inspect the trace informationIf we apply either of the options and re-run the code, we can opent the webui from the address:http://localhost:4173/sessionsThe Webui looks as the following:When we invoked the function tracing_enabled(), we optionally provided a session name: sess_test_tracing, otherwise, all the tracing information went into the default session. The tracing server doesn’t automatically detect session names. We have to manually create a new session named ‘sess_test_tracing’ to match the session name in the code.When we open the session, we can find the tracing details:If we click on the ‘Explore’ buttons, we can find all the prompt information for each interaction. For example:Answer the following questions as best you can. You have access to the following tools:Search: A search engine. Useful for when you need to answer questions about current events.'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='For example:Answer the following questions as best you can. You have access to the following tools:Search: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.Calculator: Useful for when you need to answer questions about math.Use the following format:Question: the input question you must answerThought: you should always think about what to doAction: the action to take, should be one of [Search, Calculator]Action Input: the input to the actionObservation: the result of the action... (this Thought/Action/Action Input/Observation can repeat N times)Thought: I now know the final answerFinal Answer: the final answer to the original input questionBegin!Question: How long does the fastest golf ball travel around the Earth? - Do you know the speed of the fastest golf ball? Find it out. - You also need to find out the circumference of the Earth as well. - Final answer should be in hours.Thought:Without the help of tracing, we'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='you know the speed of the fastest golf ball? Find it out. - You also need to find out the circumference of the Earth as well. - Final answer should be in hours.Thought:Without the help of tracing, we would have to look into the source code to know these details.Tracing with LangChain VisualizerBesides the LangChain tracing, I would also recommend an alternative — LangChain Visualizer. The reasons are that the LangChain tracing requires running in Docker, and that the UI functions are not well refined.Step 0: installationpip install langchain-visualizerStep 1: change the codeLangChain visualizer requires the payload to be wrapped in an async function.# from langchain_visualizer import visualize # use this when not run in Jupyterfrom langchain_visualizer.jupyter import visualizefrom langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIasync def search_agent_demo():    llm = OpenAI(temperature=0)    tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='import initialize_agent, load_toolsfrom langchain.llms import OpenAIasync def search_agent_demo():    llm = OpenAI(temperature=0)    tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)    agent = initialize_agent(        tools, llm, agent=\"zero-shot-react-description\", verbose=True    )    return agent.run(        \"How long does a plane travel around the Earth at sonic speed?\"        \" - Remember the unit of circumference of the Earth is in kilometer while the unit of sound speed is meter per second.\"        \" - You also need to convert the kilometers into meters in your calculation and the final answer should be in hours.\"        \" - Do your reasoning independently.\"     )# you don\\'t have to specify width and height# but if you do, you can change the size of the rendered windowvisualize(search_agent_demo, width=1000, height=500)The above code snippet runs in Jupyter Notebook. The result of the code run is the following: You can choose to open the link in the web browser to access'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='width=1000, height=500)The above code snippet runs in Jupyter Notebook. The result of the code run is the following: You can choose to open the link in the web browser to access the same outcome.I found LangChain Visualizer to be more elegant, and I liked the cost estimation it produced for each call.ConclusionGenerative AI (GAI) is still in its early stages of development. However, I believe that we will see more and more sophisticated applications of GAI in the near future. As these applications become more complex, it will become increasingly important for us to understand how they work.In addition to debugging, tracing can be used to improve the performance of GAI systems. By identifying areas where the system can be improved, we can make it more efficient and effective.More importantly, tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing'),\n",
       " Document(metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing is a valuable tool for engineers who are working with GAI systems. I hope you find spending time on it worthwhile.LangchainLlmGenerative Ai ToolsOpenAIChatGPT----FollowWritten by Kelvin Lu958 Followers·291 FollowingSydney-based ML engineer / data engineer @ InteliaFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x11a99ded0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='22e3a6cc-a769-489f-b440-b2894db47b9d', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing is a valuable tool for engineers who are working with GAI systems. I hope you find spending time on it worthwhile.LangchainLlmGenerative Ai ToolsOpenAIChatGPT----FollowWritten by Kelvin Lu958 Followers·291 FollowingSydney-based ML engineer / data engineer @ InteliaFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams'),\n",
       " Document(id='17dd01f9-bc05-436f-88a7-0983903d28c1', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='will run as smoothly as this one.In some cases, the agent may stray from the correct path. In these cases, the verbose mode may not produce enough information to help us understand why the agent went astray. That’s why we need tracing to zoom in and see what the agent is doing at each step.Tracing with LangChain Built-in FunctionOne way is to use the LangChain’s native tracing support. LangChain, the very popular and rapidly developing framework, does support tracing. However, the document of tracing is incomplete. I hope the following guidelines can save you some time.Step 0: prepare dependenciesLangChain tracing uses a web-server to collect agent-run information. The web-server uses port number 8000 to collect trace information and port number 4173 to serve the user interface. The web-server runs as a Docker container. Before we go any further, please make sure the langchain has been installed, Docker has been installed, and the command docker-compose is executable.Step 1: start the'),\n",
       " Document(id='4be948a9-d1d3-4105-b33c-8e093a602228', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='width=1000, height=500)The above code snippet runs in Jupyter Notebook. The result of the code run is the following: You can choose to open the link in the web browser to access the same outcome.I found LangChain Visualizer to be more elegant, and I liked the cost estimation it produced for each call.ConclusionGenerative AI (GAI) is still in its early stages of development. However, I believe that we will see more and more sophisticated applications of GAI in the near future. As these applications become more complex, it will become increasingly important for us to understand how they work.In addition to debugging, tracing can be used to improve the performance of GAI systems. By identifying areas where the system can be improved, we can make it more efficient and effective.More importantly, tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing'),\n",
       " Document(id='881cabb8-0b4a-47f8-a9a9-cf82b3df3db9', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='This will help you understand how the model works and trust the answers it gives you. In this post, I’ll show you how to trace LangChain in two different ways.The Baseline Test CaseWe have designed a simple function that can solve a complex problem: how long does it take for the fastest golf ball to travel around the Earth?The function is composed of two parts: the SERP API and LLM_Math. LLM_Math is a wrapper of ChatGPT that can do simple math calculations, while the SERP API is an interface that allows Google search.When the function is asked a question, it first tries to find the answer in its knowledge base. If it can’t find the answer, it then uses the SERP API to search Google for supporting information. If it finds the information on Google, it then uses LLM_Math to calculate the final answer.Here is a code snippet that shows how the function works:from langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIdef search_agent_demo():    llm =')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query = \"LangSmith has two usage limits: total traces and extended\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing is a valuable tool for engineers who are working with GAI systems. I hope you find spending time on it worthwhile.LangchainLlmGenerative Ai ToolsOpenAIChatGPT----FollowWritten by Kelvin Lu958 Followers·291 FollowingSydney-based ML engineer / data engineer @ InteliaFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document and Retriever chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11b8551d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11b861050>, root_client=<openai.OpenAI object at 0x11b84d550>, root_async_client=<openai.AsyncOpenAI object at 0x11b855390>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "# Here we create a chain that combines retrieved documents with a prompt template\n",
    "# to generate responses from the LLM\n",
    "\n",
    "# Import required components\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template that instructs the LLM to answer based only on provided context\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create a chain that:\n",
    "# 1. Takes the retrieved documents\n",
    "# 2. Combines them into the prompt template\n",
    "# 3. Sends to the LLM for response generation\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangSmith's usage limits include two key metrics: total traces and extended traces. These are the metrics tracked on their usage graph.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"LangSmith has two usage limits: total traces and extended\",\n",
    "        \"context\": [\n",
    "            Document(\n",
    "                page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we've been tracking on our usage graph. \"\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x11a99ded0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x11a99ded0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11b8551d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11b861050>, root_client=<openai.OpenAI object at 0x11b84d550>, root_async_client=<openai.AsyncOpenAI object at 0x11b855390>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of tracing in the context of LangChain and Generative AI systems? \\n\\nThe purpose of tracing in the context of LangChain and Generative AI systems is to improve debugging and performance, ensure transparency, and build trust with users. Tracing allows users to understand how the system operates internally, making it clear that decisions are not made in a \"black box.\" It helps identify areas where improvements can be made to make the system more efficient and effective. Additionally, it provides detailed information on the system\\'s operations, which can be valuable for engineers working with GAI systems.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response = retrieval_chain.invoke(\n",
    "    {\"input\": \"LangSmith has two usage limits: total traces and extended\"}\n",
    ")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith has two usage limits: total traces and extended',\n",
       " 'context': [Document(id='22e3a6cc-a769-489f-b440-b2894db47b9d', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing is a valuable tool for engineers who are working with GAI systems. I hope you find spending time on it worthwhile.LangchainLlmGenerative Ai ToolsOpenAIChatGPT----FollowWritten by Kelvin Lu958 Followers·291 FollowingSydney-based ML engineer / data engineer @ InteliaFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams'),\n",
       "  Document(id='17dd01f9-bc05-436f-88a7-0983903d28c1', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='will run as smoothly as this one.In some cases, the agent may stray from the correct path. In these cases, the verbose mode may not produce enough information to help us understand why the agent went astray. That’s why we need tracing to zoom in and see what the agent is doing at each step.Tracing with LangChain Built-in FunctionOne way is to use the LangChain’s native tracing support. LangChain, the very popular and rapidly developing framework, does support tracing. However, the document of tracing is incomplete. I hope the following guidelines can save you some time.Step 0: prepare dependenciesLangChain tracing uses a web-server to collect agent-run information. The web-server uses port number 8000 to collect trace information and port number 4173 to serve the user interface. The web-server runs as a Docker container. Before we go any further, please make sure the langchain has been installed, Docker has been installed, and the command docker-compose is executable.Step 1: start the'),\n",
       "  Document(id='4be948a9-d1d3-4105-b33c-8e093a602228', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='width=1000, height=500)The above code snippet runs in Jupyter Notebook. The result of the code run is the following: You can choose to open the link in the web browser to access the same outcome.I found LangChain Visualizer to be more elegant, and I liked the cost estimation it produced for each call.ConclusionGenerative AI (GAI) is still in its early stages of development. However, I believe that we will see more and more sophisticated applications of GAI in the near future. As these applications become more complex, it will become increasingly important for us to understand how they work.In addition to debugging, tracing can be used to improve the performance of GAI systems. By identifying areas where the system can be improved, we can make it more efficient and effective.More importantly, tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing'),\n",
       "  Document(id='881cabb8-0b4a-47f8-a9a9-cf82b3df3db9', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='This will help you understand how the model works and trust the answers it gives you. In this post, I’ll show you how to trace LangChain in two different ways.The Baseline Test CaseWe have designed a simple function that can solve a complex problem: how long does it take for the fastest golf ball to travel around the Earth?The function is composed of two parts: the SERP API and LLM_Math. LLM_Math is a wrapper of ChatGPT that can do simple math calculations, while the SERP API is an interface that allows Google search.When the function is asked a question, it first tries to find the answer in its knowledge base. If it can’t find the answer, it then uses the SERP API to search Google for supporting information. If it finds the information on Google, it then uses LLM_Math to calculate the final answer.Here is a code snippet that shows how the function works:from langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIdef search_agent_demo():    llm =')],\n",
       " 'answer': 'What is the purpose of tracing in the context of LangChain and Generative AI systems? \\n\\nThe purpose of tracing in the context of LangChain and Generative AI systems is to improve debugging and performance, ensure transparency, and build trust with users. Tracing allows users to understand how the system operates internally, making it clear that decisions are not made in a \"black box.\" It helps identify areas where improvements can be made to make the system more efficient and effective. Additionally, it provides detailed information on the system\\'s operations, which can be valuable for engineers working with GAI systems.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='22e3a6cc-a769-489f-b440-b2894db47b9d', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing is a valuable tool for engineers who are working with GAI systems. I hope you find spending time on it worthwhile.LangchainLlmGenerative Ai ToolsOpenAIChatGPT----FollowWritten by Kelvin Lu958 Followers·291 FollowingSydney-based ML engineer / data engineer @ InteliaFollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams'),\n",
       " Document(id='17dd01f9-bc05-436f-88a7-0983903d28c1', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='will run as smoothly as this one.In some cases, the agent may stray from the correct path. In these cases, the verbose mode may not produce enough information to help us understand why the agent went astray. That’s why we need tracing to zoom in and see what the agent is doing at each step.Tracing with LangChain Built-in FunctionOne way is to use the LangChain’s native tracing support. LangChain, the very popular and rapidly developing framework, does support tracing. However, the document of tracing is incomplete. I hope the following guidelines can save you some time.Step 0: prepare dependenciesLangChain tracing uses a web-server to collect agent-run information. The web-server uses port number 8000 to collect trace information and port number 4173 to serve the user interface. The web-server runs as a Docker container. Before we go any further, please make sure the langchain has been installed, Docker has been installed, and the command docker-compose is executable.Step 1: start the'),\n",
       " Document(id='4be948a9-d1d3-4105-b33c-8e093a602228', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='width=1000, height=500)The above code snippet runs in Jupyter Notebook. The result of the code run is the following: You can choose to open the link in the web browser to access the same outcome.I found LangChain Visualizer to be more elegant, and I liked the cost estimation it produced for each call.ConclusionGenerative AI (GAI) is still in its early stages of development. However, I believe that we will see more and more sophisticated applications of GAI in the near future. As these applications become more complex, it will become increasingly important for us to understand how they work.In addition to debugging, tracing can be used to improve the performance of GAI systems. By identifying areas where the system can be improved, we can make it more efficient and effective.More importantly, tracing can also be used to build trust with users. By showing users how the system works, we can help them understand that the system is not making decisions in a black box.In conclusion, tracing'),\n",
       " Document(id='881cabb8-0b4a-47f8-a9a9-cf82b3df3db9', metadata={'source': 'https://medium.com/@kelvin.lu.au/tracing-how-langchain-works-behind-the-scenes-c17425ed5d1d', 'title': 'Tracing How LangChain Works Behind the Scenes | by Kelvin Lu | Medium', 'description': 'Have you ever wondered how LangChain works its magic? When you ask LangChain a question, it seems to just know the answer. But how does it do that? Is the answer trustworthy, or is it just a…', 'language': 'en'}, page_content='This will help you understand how the model works and trust the answers it gives you. In this post, I’ll show you how to trace LangChain in two different ways.The Baseline Test CaseWe have designed a simple function that can solve a complex problem: how long does it take for the fastest golf ball to travel around the Earth?The function is composed of two parts: the SERP API and LLM_Math. LLM_Math is a wrapper of ChatGPT that can do simple math calculations, while the SERP API is an interface that allows Google search.When the function is asked a question, it first tries to find the answer in its knowledge base. If it can’t find the answer, it then uses the SERP API to search Google for supporting information. If it finds the information on Google, it then uses LLM_Math to calculate the final answer.Here is a code snippet that shows how the function works:from langchain.agents import initialize_agent, load_toolsfrom langchain.llms import OpenAIdef search_agent_demo():    llm =')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
